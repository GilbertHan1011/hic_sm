from pathlib import Path
import yaml
from urllib.parse import urlparse
import pandas as pd
import shlex

include: "rules/common.smk"


# configfile: "config/config.yml"


localrules:
    merge_stats_libraries_into_groups,


downloaded_fastqs_folder = config["output"]["dirs"]["downloaded_fastqs"]
processed_fastqs_folder = config["output"]["dirs"]["processed_fastqs"]
fastqc_folder = config["output"]["dirs"]["fastqc"]
mapped_parsed_sorted_chunks_folder = config["output"]["dirs"][
    "mapped_parsed_sorted_chunks"
]
pairs_runs_folder = config["output"]["dirs"]["pairs_runs"]
pairs_library_folder = config["output"]["dirs"]["pairs_library"]
coolers_library_folder = config["output"]["dirs"]["coolers_library"]
coolers_library_group_folder = config["output"]["dirs"]["coolers_library_group"]
stats_library_group_folder = config["output"]["dirs"]["stats_library_group"]

multiqc_folder = config["output"]["dirs"]["multiqc"]

assembly = config["input"]["genome"]["assembly_name"]
genome_path = config["input"]["genome"]["bwa_index_wildcard_path"].rstrip("*")
# custom_genome_folder = Path(config["genome"]["custom_genome_path"]).parent
chromsizes_path = config["input"]["genome"]["chrom_sizes_path"]


LIBRARY_RUN_FASTQS = organize_fastqs(config)
runs = [list(LIBRARY_RUN_FASTQS[lib].keys()) for lib in LIBRARY_RUN_FASTQS.keys()]
runs = [item for sublist in runs for item in sublist]

min_resolution = min(config["bin"]["resolutions"])

# Setting up FASTQC outputs if required
fastqc = []
if config.get("do_fastqc", False):
    for library in LIBRARY_RUN_FASTQS:
        fastqc += expand(
            f"{fastqc_folder}/{library}.{{run}}.{{side}}_fastqc.zip",
            run=LIBRARY_RUN_FASTQS[library].keys(),
            side=[1, 2],
        )

# Setting up scaling from pairs outputs if required
library_scaling_pairs = []
if "scaling_pairs" in config and config["scaling_pairs"].get("do", True):
    library_scaling_pairs = expand(
        f"{pairs_library_folder}/{{library}}.{assembly}.nodups.scaling.tsv",
        library=LIBRARY_RUN_FASTQS.keys(),
    )
    if not config["scaling_pairs"].get("max_distance", False):
        chromsizes = pd.read_table(
            chromsizes_path, header=None, names=["chrom", "size"]
        )
        config["scaling_pairs"]["max_distance"] = chromsizes["size"].max()

# Setting up the library cooler outputs
library_coolers = expand(
    f"{coolers_library_folder}/{{library}}.{assembly}.{{filter_name}}.{min_resolution}.mcool",
    library=LIBRARY_RUN_FASTQS.keys(),
    filter_name=list(config["bin"]["filters"].keys()),
)
library_hic = []
if config["bin"].get("make_hic", False):
    library_hic += expand(
        f"{coolers_library_folder}/{{library}}.{assembly}.{{filter_name}}.{min_resolution}.hic",
        library=LIBRARY_RUN_FASTQS.keys(),
        filter_name=list(config["bin"]["filters"].keys()),
    )

# Setting up the library group cooler and group stats outputs
library_group_coolers = []
library_group_hic = []
library_group_stats = []
# library_group_scaling_pairs = []
if "library_groups" in config["input"] and len(config["input"]["library_groups"]) > 0:
    library_group_coolers = expand(
        f"{coolers_library_group_folder}/{{library_group}}.{assembly}.{{filter_name}}.{min_resolution}.mcool",
        library_group=config["input"]["library_groups"].keys(),
        filter_name=list(config["bin"]["filters"].keys()),
    )
    if config["bin"].get("make_hic", False):
        library_group_hic = expand(
            f"{coolers_library_group_folder}/{{library_group}}.{assembly}.{{filter_name}}.{min_resolution}.hic",
            library_group=config["input"]["library_groups"].keys(),
            filter_name=list(config["bin"]["filters"].keys()),
        )
    library_group_stats = expand(
        f"{stats_library_group_folder}/{{library_group}}.{assembly}.stats",
        library_group=config["input"]["library_groups"].keys(),
    )
    # library_group_scaling_pairs = expand(
    #     f"{stats_library_group_folder}/{{library_group}}.{assembly}.scaling.tsv",
    #     library_group=config["input"]["library_groups"].keys(),
    # )

multiqc = [f"{multiqc_folder}/multiqc_report.html"]

# Setting up resgen upload outputs (empty files indicating that the upload has been done)
resgen_uploads = (
    [f"{clr}_uploaded" for clr in library_coolers + library_group_coolers]
    if config["resgen"]["upload"]
    else []
)

# Some sanity checks
# Phasing
if config.get('phase', {}).get('do_phase', False):
    if config['map']['mapper']=='chromap':
        raise ValueError(
            "Phasing is not possible with chromap, please use bwa-mem, bwa-mem2 or bwa-meme."
        )
    parse_options = argstring_to_dict(config['parse'].get('parsing_options', ''))
    if '--min-mapq' not in parse_options or parse_options['--min-mapq'] != '0':
        raise ValueError(
            "Plase set '--min-mapq to 0' in the parsing options to use phasing."
        )
    if '--add-columns' not in parse_options:
        raise ValueError(
            "Please set the appropriate --add-columns argument in the parsing options to use phasing."
        )
    elif config['phase']['tag_mode']=='XA' and not set(['XA', 'NM', 'AS', 'XS', 'mapq']).issubset(set(parse_options['--add-columns'].split(','))):
        raise ValueError(
            "Please set '--add-columns XA,NM,AS,XS,mapq' in the parsing options to use phasing with XA tag mode."
        )
    elif config['phase']['tag_mode']=='XB' and not set(['XB', 'NM', 'AS', 'XS', 'mapq']).issubset(set(parse_options['--add-columns'].split(','))):
        raise ValueError(
            "Please set '--add-columns XB,NM,AS,XS,mapq' in the parsing options to use phasing with XB tag mode."
        )
    # simpler check, should be enough and in case of multiple extra col pairs, where the dict would only record the last one...
    dedup_options = config["dedup"].get("dedup_options", "") 
    if '--extra-col-pair phase1 phase1' not in dedup_options or '--extra-col-pair phase2 phase2' not in dedup_options:
        raise ValueError(
            "Please add '--extra-col-pair phase1 phase2' in the dedup options to use phasing."
        )


# List of all required outputs of the workflow
# Note that pairs are not listed, since they are needed to produce coolers, so they are
# implicitly required
rule default:
    input:
        library_group_coolers,
        library_group_hic,
        library_group_stats,
        library_coolers,
        library_hic,
        library_scaling_pairs,
        # library_group_scaling_pairs,
        fastqc,
        multiqc,
        resgen_uploads,


# Global constraints on the wildcards to be used in the workflow - ensures correct
# parsing of all file names into wildcards
wildcard_constraints:
    library=f"({'|'.join([re.escape(lib) for lib in LIBRARY_RUN_FASTQS.keys()])})",
    library_group=(
        f"({'|'.join([re.escape(lib) for lib in config['input']['library_groups'].keys()])})"
        if "library_groups" in config["input"]
        else ""
    ),
    run=f"({'|'.join([re.escape(run) for run in runs])})",
    chunk_id="[0-9]+",


# fasterq-dump just doesn't work for me on our server for some reason, but this code
# should work as an alternative
# Annoyingly, fasterq-dump doesn't have --gzip argument :(

# fasterq_dump_extra = {}
# fastq_accession_dict = {}
# for library in LIBRARY_RUN_FASTQS:
#     fastq_accession_dict[library] = {}
#     for run in LIBRARY_RUN_FASTQS[library]:
#         paths = LIBRARY_RUN_FASTQS[library][run]
#         if len(paths) == 1 and paths[0].startswith("sra:"):
#             parsed = urlparse(paths[0])
#             srr, query = parsed.path, parsed.query
#             if query:
#                 start, end = get_start_end(query)
#                 fasterq_dump_extra[srr] = f"--minSpotId {start}" if start else ""
#                 fasterq_dump_extra[srr] += f" --maxSpotId {end}" if end else ""
#             fastq_accession_dict[library][run] = srr


# rule fasterq_dump:
#     output:
#         # the wildcard name must be accession, pointing to an SRA number
#         temp(f"{downloaded_fastqs_folder}/{{accession}}_1.fastq.gz"),
#         temp(f"{downloaded_fastqs_folder}/{{accession}}_2.fastq.gz"),
#     log:
#         "logs/fasterq_dump/{accession}.log",
#     params:
#         extra=lambda wildcards: fasterq_dump_extra[wildcards.accession],
#     threads: 4
#     wrapper:
#         "v4.6.0/bio/sra-tools/fasterq-dump"


# rule rename_downloaded_fastq:
#     input:
#         fastq1=lambda wildcards: f"{downloaded_fastqs_folder}/{fastq_accession_dict[wildcards.library][wildcards.run]}_1.fastq.gz",
#         fastq2=lambda wildcards: f"{downloaded_fastqs_folder}/{fastq_accession_dict[wildcards.library][wildcards.run]}_2.fastq.gz",
#     output:
#         fastq1=f"{downloaded_fastqs_folder}/{{library}}.{{run}}.1.fastq.gz",
#         fastq2=f"{downloaded_fastqs_folder}/{{library}}.{{run}}.2.fastq.gz",
#     shell:
#         """
#         mv {input.fastq1} {output.fastq1}
#         mv {input.fastq2} {output.fastq2}
#         """


rule download_fastqs:
    output:
        fastq1=f"{downloaded_fastqs_folder}/{{library}}.{{run}}.1.fastq.gz",
        fastq2=f"{downloaded_fastqs_folder}/{{library}}.{{run}}.2.fastq.gz",
    log:
        "logs/download_fastqs/{library}.{run}.log",
    threads: 8
    conda:
        "envs/download_fastqs.yml"
    params:
        bgzip_threads=lambda wildcards, threads: max(1, (threads - 2) / 2),
        fastq_files=lambda wildcards: LIBRARY_RUN_FASTQS[wildcards.library][
            wildcards.run
        ],
    script:
        "scripts/download_split_fastq.py"


# Chunking command, depending on whether chunking is required - simply copying the files
# if no chunking needed, or splitting them into chunks of required size
if config["map"]["chunksize"] > 0:
    chunk_command = """
        mkdir -p {output};
        zcat {input.fastq1} | split -l {params.chunksize} -d \
            --filter 'bgzip -c -@ {threads} > $FILE.fastq.gz' - \
            {output}/1.
        zcat {input.fastq2} | split -l {params.chunksize} -d \
            --filter 'bgzip -c -@ {threads} > $FILE.fastq.gz' - \
            {output}/2.
        """
else:
    chunk_command = """
        mkdir -p {output}
        cp {input.fastq1} {output}/1.00.fastq.gz
        cp {input.fastq2} {output}/2.00.fastq.gz
        """


checkpoint chunk_fastq:
    input:
        fastq1=lambda wildcards: (
            f"{downloaded_fastqs_folder}/{wildcards.library}.{wildcards.run}.1.fastq.gz"
            if needs_downloading(
                LIBRARY_RUN_FASTQS[wildcards.library][wildcards.run], 0
            )
            else LIBRARY_RUN_FASTQS[wildcards.library][wildcards.run][0]
        ),
        fastq2=lambda wildcards: (
            f"{downloaded_fastqs_folder}/{wildcards.library}.{wildcards.run}.2.fastq.gz"
            if needs_downloading(
                LIBRARY_RUN_FASTQS[wildcards.library][wildcards.run], 1
            )
            else LIBRARY_RUN_FASTQS[wildcards.library][wildcards.run][1]
        ),
    params:
        chunksize=lambda wildcards: config["map"]["chunksize"] * 4,  #*4 because we split by lines, not by reads
    threads: lambda wildcards: 4 if config["map"]["chunksize"] > 0 else 1
    conda:
        lambda wildcards: (
            "envs/download_fastqs.yml" if config["map"]["chunksize"] > 0 else None
        )
    output:
        directory(f"{processed_fastqs_folder}/{{library}}/{{run}}"),
    log:
        "logs/chunk_runs/{library}.{run}.tsv",
    benchmark:
        "benchmarks/chunk_runs/{library}.{run}.tsv"
    shell:
        chunk_command


rule fastqc:
    input:
        lambda wildcards: (
            f"downloaded_fastqs/{wildcards.library}.{wildcards.run}.{wildcards.side}.fastq.gz"
            if needs_downloading(
                LIBRARY_RUN_FASTQS[wildcards.library][wildcards.run],
                int(wildcards.side) - 1,
            )
            else LIBRARY_RUN_FASTQS[wildcards.library][wildcards.run][
                int(wildcards.side) - 1
            ]
        ),
    output:
        html=f"{fastqc_folder}/{{library}}.{{run}}.{{side}}_fastqc.html",
        zip=f"{fastqc_folder}/{{library}}.{{run}}.{{side}}_fastqc.zip",
    log:
        "logs/fastqc/{library}.{run}.{side}.log",
    benchmark:
        "benchmarks/fastqc/{library}.{run}.{side}.tsv"
    threads: 1
    wrapper:
        "v4.6.0/bio/fastqc"


# Depending on the mapper, the index files will be different
if config["map"]["mapper"] == "bwa-mem":
    idx = multiext(
        genome_path,
        ".amb",
        ".ann",
        ".bwt",
        ".pac",
        ".sa",
    )

elif config["map"]["mapper"] == "bwa-mem2":
    idx = multiext(
        genome_path,
        ".0123",
        ".amb",
        ".ann",
        ".bwt.2bit.64",
        ".pac",
    )

else:  # bwa-meme
    idx = multiext(
        genome_path,
        ".0123",
        ".amb",
        ".ann",
        ".pac",
        ".pos_packed",
        ".suffixarray_uint64",
        ".suffixarray_uint64_L0_PARAMETERS",
        ".suffixarray_uint64_L1_PARAMETERS",
        ".suffixarray_uint64_L2_PARAMETERS",
    )

if config["map"]["mapper"] == "chromap":

    ruleorder: map_chunks_chromap > parse_sort_chunks

else:

    ruleorder: parse_sort_chunks > map_chunks_chromap


rule bwaindex:
    input:
        genome=genome_path,
    output:
        idx=idx,
    params:
        bwa=config["map"]["mapper"],
    threads: 1  #Only affects bwa-meme
    log:
        f"logs/bwa-memx_index/{assembly}.log",
    cache: True
    wrapper:
        "v4.6.0/bio/bwa-memx/index"


rule chromap_index:
    input:
        genome=genome_path,
    output:
        idx=multiext(genome_path, ".chromap.index"),
    log:
        f"logs/chromap_index/{assembly}.log",
    conda:
        "envs/chromap.yml"
    shell:
        r"chromap -i -r {input.genome} -o {output.idx} >{log} 2>&1"


# faidx doesn't work with gzipped files :( So test fails, but probably should work
# with .fa, need to check
# rule faidx:
#     input:
#         genome_custom=config["input"]["genome"]["genome_fasta_path"],
#     output:
#         genome_faidx=f"{config['input']['genome']['genome_fasta_path']}.fai",
#     log:
#         f"logs/faidx/{assembly}.log",
#     params:
#         extra="",  # optional params string
#     wrapper:
#         "v4.6.0/bio/samtools/faidx"


# rule make_chromsizes:
#     input:
#         genome_faidx=f"{config['input']['genome']['genome_fasta_path']}.fai",
#     output:
#         chromsizes=chromsizes_path,
#     shell:
#         "cut -f1,2 {input.genome_faidx} > {output.chromsizes}"


rule trim:
    input:
        sample=[
            f"{processed_fastqs_folder}/{{library}}/{{run}}/1.{{chunk_id}}.fastq.gz",
            f"{processed_fastqs_folder}/{{library}}/{{run}}/2.{{chunk_id}}.fastq.gz",
        ],
    log:
        "logs/fastp/{library}.{run}.{chunk_id}.log",
    params:
        extra=config["map"]["trim_options"],
    output:
        # Would be better with a pipe, but it causes weird freezing of the pipeline
        trimmed=[
            temp(
                f"{processed_fastqs_folder}/{{library}}/{{run}}/1.{{chunk_id}}_trimmed.fastq.gz"
            ),
            temp(
                f"{processed_fastqs_folder}/{{library}}/{{run}}/2.{{chunk_id}}_trimmed.fastq.gz"
            ),
        ],
        json=f"{mapped_parsed_sorted_chunks_folder}/{{library}}/{{run}}/{{chunk_id}}.fastp.json",
        html=f"{mapped_parsed_sorted_chunks_folder}/{{library}}/{{run}}/{{chunk_id}}.fastp.html",
    wrapper:
        "v4.6.0/bio/fastp"


# If we keep bams, simply store the output in the folder, otherwise pipe it to pairtools parse
# Perhaps something with tee might be faster, had trouble getting it to work
if config["parse"]["keep_unparsed_bams"]:

    ruleorder: map_chunks_bwa > map_chunks_bwa_pipe

else:

    ruleorder: map_chunks_bwa_pipe > map_chunks_bwa


rule map_chunks_bwa:
    input:
        reads=lambda wildcards: (
            [
                f"{processed_fastqs_folder}/{{library}}/{{run}}/1.{{chunk_id}}_trimmed.fastq.gz",
                f"{processed_fastqs_folder}/{{library}}/{{run}}/2.{{chunk_id}}_trimmed.fastq.gz",
            ]
            if config["map"]["trim_options"]
            else [
                f"{processed_fastqs_folder}/{{library}}/{{run}}/1.{{chunk_id}}.fastq.gz",
                f"{processed_fastqs_folder}/{{library}}/{{run}}/2.{{chunk_id}}.fastq.gz",
            ]
        ),
        reference=genome_path,
        idx=idx,
    params:
        bwa=config["map"]["mapper"],
        extra="-SP",
        sort="none",
        dedup="none",
    threads: 4
    output:
        f"{mapped_parsed_sorted_chunks_folder}/{{library}}/{{run}}/{{chunk_id}}.bam",
    log:
        "logs/bwa_memx/{library}.{run}.{chunk_id}.log",
    benchmark:
        "benchmarks/bwa_memx/{library}.{run}.{chunk_id}.tsv"
    wrapper:
        "v4.6.0/bio/bwa-memx/mem"


use rule map_chunks_bwa as map_chunks_bwa_pipe with:
    output:
        pipe(
            f"{mapped_parsed_sorted_chunks_folder}/{{library}}/{{run}}/{{chunk_id}}.bam"
        ),


rule parse_sort_chunks:
    input:
        bam=f"{mapped_parsed_sorted_chunks_folder}/{{library}}/{{run}}/{{chunk_id}}.bam",
        chromsizes=chromsizes_path,
    threads: 4
    params:
        # keep_bams_command=f"| tee >(samtools view -bS > {mapped_parsed_sorted_chunks_folder}/{{library}}/{{run}}/{{chunk_id}}.{assembly}.bam)"
        # if config["parse"]["keep_unparsed_bams"]
        # else "",
        dropsam_flag="" if config["parse"].get("make_pairsam", False) else "--drop-sam",
        dropreadid_flag=(
            "--drop-readid" if config["parse"].get("drop_readid", False) else ""
        ),
        dropseq_flag="--drop-seq" if config["parse"].get("drop_seq", True) else "",
        parsing_options=config["parse"].get("parsing_options", ""),
    conda:
        "envs/pairtools_cooler.yml"
    output:
        f"{mapped_parsed_sorted_chunks_folder}/{{library}}/{{run}}/{{chunk_id}}.{assembly}.pairs.gz",
    benchmark:
        "benchmarks/parse_sort_chunks/{library}.{run}.{chunk_id}.tsv"
    log:
        "logs/parse_sort_chunks/{library}.{run}.{chunk_id}.log",
    shell:
        r"""
        pairtools parse {params.dropsam_flag} {params.dropreadid_flag} {params.dropseq_flag} \
        {params.parsing_options} \
        -c {input.chromsizes} {input.bam} \
        | pairtools sort --nproc {threads} -o {output} \
        >{log[0]} 2>&1
        """


rule map_chunks_chromap:
    input:
        reads=lambda wildcards: (
            [
                f"{processed_fastqs_folder}/{{library}}/{{run}}/1.{{chunk_id}}_trimmed.fastq.gz",
                f"{processed_fastqs_folder}/{{library}}/{{run}}/2.{{chunk_id}}_trimmed.fastq.gz",
            ]
            if config["map"]["trim_options"]
            else [
                f"{processed_fastqs_folder}/{{library}}/{{run}}/1.{{chunk_id}}.fastq.gz",
                f"{processed_fastqs_folder}/{{library}}/{{run}}/2.{{chunk_id}}.fastq.gz",
            ]
        ),
        reference=genome_path,
        idx=multiext(genome_path, ".chromap.index"),
    params:
        extra=config["map"].get("mapping_options", ""),
    threads: 8
    output:
        f"{mapped_parsed_sorted_chunks_folder}/{{library}}/{{run}}/{{chunk_id}}.{assembly}.pairs.gz",
    log:
        "logs/chromap/{library}.{run}.{chunk_id}.log",
    benchmark:
        "benchmarks/chromap/{library}.{run}.{chunk_id}.tsv"
    conda:
        "envs/chromap.yml"
    shell:
        # chromap doesn't output gzip files, so we need to pipe it to bgzip
        # It doesn't work with low memory mode, so we can't use the hic preset
        # Hence I provide all arguments manually except for --low-mem
        r"""
        chromap -e 4 -q 1 --split-alignment --pairs -x {input.idx} -r {input.reference} \
        -t {threads} {params.extra} \
        -1 {input.reads[0]} -2 {input.reads[1]} -o /dev/stdout 2>{log} | \
        bgzip > {output} \
        """


# Find out the chunk ids for each library and run - since we don't know them beforehand
def get_pair_chunks(wildcards):
    checkpoint_output = checkpoints.chunk_fastq.get(**wildcards).output
    chunk_ids = glob_wildcards(
        f"{processed_fastqs_folder}/{wildcards.library}/{wildcards.run}/2.{{chunk_id}}.fastq.gz",
    ).chunk_id
    paths = expand(
        f"{mapped_parsed_sorted_chunks_folder}/{wildcards.library}/{wildcards.run}/{{chunk_id}}.{assembly}.pairs.gz",
        chunk_id=chunk_ids,
    )
    return paths


rule merge_runs:
    input:
        get_pair_chunks,
    threads: 4
    conda:
        "envs/pairtools_cooler.yml"
    output:
        f"{pairs_runs_folder}/{{library}}/{{run}}.{assembly}.pairs.gz",
    log:
        "logs/merge_runs/{library}.{run}.log",
    benchmark:
        "benchmarks/merge_runs/{library}.{run}.tsv"
    params:
        command=lambda wildcards, input, threads, output: (
            f"pairtools merge {input} --nproc {threads} -o {output}"
            if len(input) > 1
            else f"cp {input} {output}"
        ),
    shell:
        r"""{params.command} \
        >{log[0]} 2>&1
        """


if config["parse"]["make_pairsam"]:
    bytile_arg = (
        "--keep-parent-id --output-bytile-stats {output[7]}"
        if config["dedup"].get("save_by_tile_dups", False)
        else ""
    )
    dedup_command = (
        """pairtools dedup {params.dedup_options} \
        --max-mismatch {params.max_mismatch_bp} \
        --mark-dups \
        --output \
            >( pairtools split \
                --output-pairs {output[0]} \
                --output-sam {output[1]} \
             ) \
        --output-unmapped \
            >( pairtools split \
                --output-pairs {output[2]} \
                --output-sam {output[3]} \
             ) \
        --output-dups \
            >( pairtools split \
                --output-pairs {output[4]} \
                --output-sam {output[5]} \
             ) \
        --output-stats {output[6]} \
        """
        + bytile_arg
        + " && pairix {output[0]}"
    )
    merge_output = multiext(
        f"{pairs_library_folder}/{{library}}.{assembly}",
        ".nodups.pairs.gz",
        ".nodups.bam",
        ".unmapped.pairs.gz",
        ".unmapped.bam",
        ".dups.pairs.gz",
        ".dups.bam",
        ".dedup.stats",
        ".nodups.pairs.gz.px2",
    )
else:
    bytile_arg = (
        "--keep-parent-id --output-bytile-stats {output[5]}"
        if config["dedup"].get("save_by_tile_dups", False)
        else ""
    )
    dedup_command = (
        """pairtools dedup {params.dedup_options} \
        --max-mismatch {params.max_mismatch_bp} \
        --mark-dups \
        --output {output[0]} \
        --output-unmapped {output[1]} \
        --output-dups {output[2]} \
        --output-stats {output[3]} \
        """
        + bytile_arg
        + " >{log[0]} 2>&1 && pairix {output[0]}"
    )
    merge_output = multiext(
        f"{pairs_library_folder}/{{library}}.{assembly}",
        ".nodups.pairs.gz",
        ".unmapped.pairs.gz",
        ".dups.pairs.gz",
        ".dedup.stats",
        ".nodups.pairs.gz.px2",
    )
if config["dedup"].get("save_by_tile_dups", False):
    merge_output += [f"{pairs_library_folder}/{{library}}.{assembly}.by_tile_dups.txt"]


rule merge_dedup:
    input:
        pairs=lambda wildcards: expand(
            f"{pairs_runs_folder}/{wildcards.library}/{{run}}.{assembly}.pairs.gz",
            run=list(LIBRARY_RUN_FASTQS[wildcards.library].keys()),
        ),
    params:
        dedup_options=lambda wildcards: config["dedup"].get("dedup_options", ""),
        max_mismatch_bp=config["dedup"]["max_mismatch_bp"],
        merge_command=lambda wildcards, input, threads: (
            f"pairtools merge {input} --nproc {threads} | "
            if len(input) > 1
            else f"bgzip -dc -@ {threads-1} {input} | "
        ),
        phase_command=lambda wildcards, threads: (
            f'pairtools phase --tag-mode {config["phase"]["tag_mode"]} --phase-suffixes {" ".join(config["phase"]["suffixes"])} | pairtools sort --nproc {threads - 1} | '
            if config.get("phase", {}).get("do_phase", False)
            else ''
        ),
    threads: 4
    conda:
        "envs/pairtools_cooler.yml"
    output:
        merge_output,
    log:
        "logs/merge_dedup/{library}.log",
    benchmark:
        "benchmarks/merge_dedup/{library}.tsv"
    shell:
        r"{params.merge_command}" + r"{params.phase_command}" + dedup_command + " >{log[0]} 2>&1"


# balance_args = "--balance" if {config["bin"].get("balance", True)} else ""
# balance_options = config["bin"].get("balance_options", "")
# if balance_args and balance_options:
#     balance_args = f"{balance_args} {balance_options}"


rule bin_pairs_library:
    input:
        pairs=f"{pairs_library_folder}/{{library}}.{assembly}.nodups.pairs.gz",
        chromsizes=chromsizes_path,
    params:
        filter_command=lambda wildcards: (
            f'| pairtools select "{config["bin"]["filters"][wildcards.filter_name]}"'
            if config["bin"]["filters"][wildcards.filter_name]
            else ""
        ),
        assembly=assembly,
    threads: 4
    conda:
        "envs/pairtools_cooler.yml"
    output:
        cool=f"{coolers_library_folder}/{{library}}.{assembly}.{{filter_name}}.{{min_resolution}}.cool",
    log:
        "logs/bin_pairs_library/{library}.{filter_name}.{min_resolution}.log",
    benchmark:
        "benchmarks/bin_pairs_library/{library}.{filter_name}.{min_resolution}.tsv"
    shell:
        r"""
        bgzip -dc -@ {threads} {input.pairs} {params.filter_command} | cooler cload pairs -c1 2 -p1 3 -c2 4 -p2 5 \
        --assembly {params.assembly} \
        {input.chromsizes}:{wildcards.min_resolution} - {output.cool} >{log[0]} 2>&1
        """


rule zoom_library:
    input:
        cool=f"{coolers_library_folder}/{{library}}.{assembly}.{{filter_name}}.{{min_resolution}}.cool",
    params:
        res_string=",".join([str(res) for res in config["bin"]["resolutions"]]),
        balance_args=lambda wildcards, threads: (
            f"--balance --balance-args '{config['bin'].get('balance_options', '')} --nproc {threads}'"
            if config["bin"]["balance"]
            else ""
        ),
    threads: 8
    conda:
        "envs/pairtools_cooler.yml"
    output:
        mcool=f"{coolers_library_folder}/{{library}}.{assembly}.{{filter_name}}.{{min_resolution}}.mcool",
    log:
        "logs/zoom_library/{library}.{filter_name}.{min_resolution}.log",
    benchmark:
        "benchmarks/zoom_library/{library}.{filter_name}.{min_resolution}.tsv"
    shell:
        r"""
        cooler zoomify \
        --nproc {threads} \
        --out {output.mcool} \
        --resolutions {params.res_string} \
        {params.balance_args} \
        {input.cool} \
        >{log[0]} 2>&1
        """


rule merge_zoom_library_group_coolers:
    input:
        lambda wildcards: expand(
            f"{coolers_library_folder}/{{library}}.{assembly}.{wildcards.filter_name}.{wildcards.min_resolution}.cool",
            library=config["input"]["library_groups"][wildcards.library_group],
        ),
    params:
        res_string=",".join([str(res) for res in config["bin"]["resolutions"]]),
        balance_args=lambda wildcards, threads: (
            f"--balance --balance-args '{config['bin'].get('balance_options', '')} --nproc {threads}'"
            if config["bin"]["balance"]
            else ""
        ),
    threads: 8
    conda:
        "envs/pairtools_cooler.yml"
    output:
        cool=f"{coolers_library_group_folder}/{{library_group}}.{assembly}.{{filter_name}}.{{min_resolution}}.cool",
        mcool=f"{coolers_library_group_folder}/{{library_group}}.{assembly}.{{filter_name}}.{{min_resolution}}.mcool",
    log:
        "logs/merge_zoom_library_group_coolers/{library_group}.{filter_name}.{min_resolution}.log",
    benchmark:
        "benchmarks/merge_zoom_library_group_coolers/{library_group}.{filter_name}.{min_resolution}.tsv"
    shell:
        r"""
        cooler merge {output.cool} {input} >{log[0]} 2>&1

        cooler zoomify \
        --nproc {threads} \
        --out {output.mcool} \
        --resolutions {params.res_string} \
        {params.balance_args} \
        {output.cool} \
        >{log[0]} 2>&1
        """


rule mcool2hic_group:
    input:
        mcool=f"{coolers_library_group_folder}/{{library_group}}.{assembly}.{{filter_name}}.{{min_resolution}}.mcool",
    output:
        hic=f"{coolers_library_group_folder}/{{library_group}}.{assembly}.{{filter_name}}.{{min_resolution}}.hic",
    log:
        "logs/mcool2hic/{library_group}.{filter_name}.{min_resolution}.log",
    benchmark:
        "benchmarks/mcool2hic/{library_group}.{filter_name}.{min_resolution}.tsv"
    conda:
        "envs/hictk.yml"
    threads: 8
    params:
        tmpdir=coolers_library_group_folder,
    shell:
        r"""
        hictk convert --threads {threads} --tmpdir {params.tmpdir} \
        {input.mcool} {output.hic} >{log} 2>&1
        """


use rule mcool2hic_group as mcool2hic_library with:
    input:
        mcool=f"{coolers_library_folder}/{{library}}.{assembly}.{{filter_name}}.{{min_resolution}}.mcool",
    output:
        hic=f"{coolers_library_folder}/{{library}}.{assembly}.{{filter_name}}.{{min_resolution}}.hic",
    log:
        "logs/mcool2hic/{library}.{filter_name}.{min_resolution}.log",
    benchmark:
        r"benchmarks/mcool2hic/{library}.{filter_name}.{min_resolution}.tsv"


rule scaling_pairs_library:
    input:
        pairs=f"{pairs_library_folder}/{{library}}.{assembly}.nodups.pairs.gz",
        chromsizes=chromsizes_path,
    params:
        min_distance=config["scaling_pairs"]["min_distance"],
        max_distance=config["scaling_pairs"]["max_distance"],
        n_dist_bins_decade=config["scaling_pairs"]["n_dist_bins_decade"],
        extra=config["scaling_pairs"]["scaling_options"],
    threads: 4
    conda:
        "envs/pairtools_cooler.yml"
    output:
        scaling=f"{pairs_library_folder}/{{library}}.{assembly}.nodups.scaling.tsv",
    log:
        "logs/scaling_pairs_library/{library}.log",
    benchmark:
        "benchmarks/scaling_pairs_library/{library}.tsv"
    shell:
        r"""
        pairtools scaling \
        --dist-range {params.min_distance} {params.max_distance} \
        --n-dist-bins-decade {params.n_dist_bins_decade} \
        {params.extra} \
        -o {output.scaling} \
        {input.pairs} \
        >{log[0]} 2>&1
        """


# use rule scaling_pairs_library as scaling_pairs_library_group with:
#     input:
#         pairs=f"{stats_library_group_folder}/{{library_group}}.{assembly}.pairs.gz",
#         chromsizes=chromsizes_path,
#     output:
#         scaling=f"{stats_library_group_folder}/{{library_group}}.{assembly}.scaling.tsv",
#     log:
#         "logs/scaling_pairs_library_group/{library_group}.log",
#     benchmark:
#         "benchmarks/scaling_pairs_library_group/{library_group}.tsv"


rule merge_stats_libraries_into_groups:
    input:
        lambda wildcards: expand(
            f"{pairs_library_folder}/{{library}}.{assembly}.dedup.stats",
            library=config["input"]["library_groups"][wildcards.library_group],
        ),
    conda:
        "envs/pairtools_cooler.yml"
    log:
        "logs/merge_stats_libraries_into_groups/{library_group}.log",
    output:
        f"{stats_library_group_folder}/{{library_group}}.{assembly}.stats",
    shell:
        r"pairtools stats --merge {input} -o {output} >{log[0]} 2>&1"


rule multiqc:
    input:
        fastqc,
        expand(
            f"{pairs_library_folder}/{{library}}.{assembly}.dedup.stats",
            library=config["input"]["raw_reads_paths"].keys(),
        ),
        expand(
            f"{stats_library_group_folder}/{{library_group}}.{assembly}.stats",
            library_group=config["input"]["library_groups"].keys(),
        )
        if "library_groups" in config["input"]
        else [],
    conda:
        "envs/multiqc.yml"
    log:
        "logs/multiqc.log",
    params:
        input_dirs=lambda wildcards, input: list(set([Path(f).parent for f in input])),
        outdir=lambda wildcards, output: Path(output[0]).parent,
    output:
        report=f"{multiqc_folder}/multiqc_report.html",
        dir=directory(multiqc_folder),
    shell:
        r"""multiqc -f --outdir {output.dir} -m pairtools -m fastqc \
        {params.input_dirs} \
        >{log} 2>&1"""


rule resgen_upload_library_group:
    input:
        mcool=f"{coolers_library_group_folder}/{{library_group}}.{assembly}.{{filter_name}}.{{min_resolution}}.mcool",
    conda:
        "envs/resgen_python.yml"
    log:
        f"logs/resgen_upload_library_group/{{library_group}}.{assembly}.{{filter_name}}.{{min_resolution}}.log",
    params:
        user=config["resgen"]["user"],
        project=config["resgen"]["project"],
        assembly=assembly,
    output:
        touch(
            f"{coolers_library_group_folder}/{{library_group}}.{assembly}.{{filter_name}}.{{min_resolution}}.mcool_uploaded"
        ),
    shell:
        r"""resgen sync datasets '{params.user}' '{params.project}' {input.mcool} \
        --tag filetype:cooler --tag assembly:{params.assembly} --tag datatype:matrix \
        --force-update true \
        >{log[0]} 2>&1
        """


use rule resgen_upload_library_group as resgen_upload_library with:
    input:
        mcool=f"{coolers_library_folder}/{{library}}.{assembly}.{{filter_name}}.{{min_resolution}}.mcool",
    log:
        f"logs/resgen_upload_library/{{library}}.{assembly}.{{filter_name}}.{{min_resolution}}.log",
    output:
        touch(
            f"{coolers_library_folder}/{{library}}.{assembly}.{{filter_name}}.{{min_resolution}}.mcool_uploaded"
        ),
