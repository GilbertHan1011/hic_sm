from pathlib import Path
import yaml
from urllib.parse import urlparse
import pandas as pd
import shlex

include: "rules/common.smk"

configfile: "config/test_config.yml"

#configfile: "config/test_config.yml"

# Load FASTQ samples from CSV if specified
import os
if config["input"].get("load_csv", False):
    csv_file = config["input"].get("fastq_csv", "samples.csv")
    if os.path.exists(csv_file):
        print(f"Loading FASTQ samples from CSV: {csv_file}")
        fastq_df = pd.read_csv(csv_file)
        config["input"]["raw_reads_paths"] = fastq_df
    else:
        raise FileNotFoundError(f"CSV file not found: {csv_file}. Please check the path in config.yml")
else:
    print("Using FASTQ samples from config.yml raw_reads_paths")

localrules:
    merge_stats_libraries_into_groups,


downloaded_fastqs_folder = config["output"]["dirs"]["downloaded_fastqs"]
processed_fastqs_folder = config["output"]["dirs"]["processed_fastqs"]
fastqc_folder = config["output"]["dirs"]["fastqc"]
mapped_parsed_sorted_chunks_folder = config["output"]["dirs"][
    "mapped_parsed_sorted_chunks"
]

pairs_runs_folder = config["output"]["dirs"]["pairs_runs"]
pairs_library_folder = config["output"]["dirs"]["pairs_library"]
coolers_library_folder = config["output"]["dirs"]["coolers_library"]
coolers_library_group_folder = config["output"]["dirs"]["coolers_library_group"]
stats_library_group_folder = config["output"]["dirs"]["stats_library_group"]
downstream_dist_folder = config["output"]["dirs"]["downstream_dist_folder"]


multiqc_folder = config["output"]["dirs"]["multiqc"]

assembly = config["input"]["genome"]["assembly_name"]
genome_path = config["input"]["genome"]["bwa_index_wildcard_path"].rstrip("*")
bowtie_index_path = config["input"]["genome"]["bowtie_index_path"].rstrip("*")
# custom_genome_folder = Path(config["genome"]["custom_genome_path"]).parent
chromsizes_path = config["input"]["genome"]["chrom_sizes_path"]


LIBRARY_RUN_FASTQS = organize_fastqs(config)

# Store sample metadata (e.g., ligation_site) if loading from CSV
SAMPLE_METADATA = {}
if config["input"].get("load_csv", False):
    csv_file = config["input"].get("fastq_csv", "samples.csv")
    if os.path.exists(csv_file):
        fastq_df = pd.read_csv(csv_file)
        # Store metadata for each library/run combination
        for _, row in fastq_df.iterrows():
            library = str(row['sample_id'])
            run = str(row['lane'])
            if library not in SAMPLE_METADATA:
                SAMPLE_METADATA[library] = {}
            SAMPLE_METADATA[library][run] = {}
            # Store ligation_site if available
            if 'ligation_site' in row and pd.notna(row['ligation_site']):
                SAMPLE_METADATA[library][run]['ligation_site'] = str(row['ligation_site'])
            if 'skip_ligation' in row and pd.notna(row['skip_ligation']):
                SAMPLE_METADATA[library][run]['skip_ligation'] = bool(row['skip_ligation'])

# Automatically create library_groups if all_group is True
if config["input"].get("all_group", False):
    all_libraries = list(LIBRARY_RUN_FASTQS.keys())
    if "library_groups" not in config["input"]:
        config["input"]["library_groups"] = {}
    config["input"]["library_groups"]["all"] = all_libraries
    print(f"Auto-generated 'all' group with {len(all_libraries)} libraries: {all_libraries}")
runs = [list(LIBRARY_RUN_FASTQS[lib].keys()) for lib in LIBRARY_RUN_FASTQS.keys()]
runs = [item for sublist in runs for item in sublist]

min_resolution = min(config["bin"]["resolutions"])

# Setting up FASTQC outputs if required
fastqc = []
if config.get("do_fastqc", False):
    for library in LIBRARY_RUN_FASTQS:
        fastqc += expand(
            f"{fastqc_folder}/{library}.{{run}}.{{side}}_fastqc.zip",
            run=LIBRARY_RUN_FASTQS[library].keys(),
            side=[1, 2],
        )

# Setting up scaling from pairs outputs if required
library_scaling_pairs = []
if "scaling_pairs" in config and config["scaling_pairs"].get("do", True):
    library_scaling_pairs = expand(
        f"{pairs_library_folder}/{{library}}.{assembly}.nodups.scaling.tsv",
        library=LIBRARY_RUN_FASTQS.keys(),
    )
    if not config["scaling_pairs"].get("max_distance", False):
        chromsizes = pd.read_table(
            chromsizes_path, header=None, names=["chrom", "size"]
        )
        config["scaling_pairs"]["max_distance"] = chromsizes["size"].max()

# Setting up the library cooler outputs
library_coolers = expand(
    f"{coolers_library_folder}/{{library}}.{assembly}.{{filter_name}}.{min_resolution}.mcool",
    library=LIBRARY_RUN_FASTQS.keys(),
    filter_name=list(config["bin"]["filters"].keys()),
)

library_downstream_stat = expand(
    f"{downstream_dist_folder}/{{library}}.{{filter_name}}.{min_resolution}_loglog_fits.csv",
    library=LIBRARY_RUN_FASTQS.keys(),
    filter_name=list(config["bin"]["filters"].keys()),
)

library_hic = []
if config["bin"].get("make_hic", False):
    library_hic += expand(
        f"{coolers_library_folder}/{{library}}.{assembly}.{{filter_name}}.{min_resolution}.hic",
        library=LIBRARY_RUN_FASTQS.keys(),
        filter_name=list(config["bin"]["filters"].keys()),
    )

# Setting up the library group cooler and group stats outputs
library_group_coolers = []
library_group_hic = []
library_group_stats = []
# library_group_scaling_pairs = []
if "library_groups" in config["input"] and len(config["input"]["library_groups"]) > 0:
    library_group_coolers = expand(
        f"{coolers_library_group_folder}/{{library_group}}.{assembly}.{{filter_name}}.{min_resolution}.mcool",
        library_group=config["input"]["library_groups"].keys(),
        filter_name=list(config["bin"]["filters"].keys()),
    )
    if config["bin"].get("make_hic", False):
        library_group_hic = expand(
            f"{coolers_library_group_folder}/{{library_group}}.{assembly}.{{filter_name}}.{min_resolution}.hic",
            library_group=config["input"]["library_groups"].keys(),
            filter_name=list(config["bin"]["filters"].keys()),
        )
    library_group_stats = expand(
        f"{stats_library_group_folder}/{{library_group}}.{assembly}.stats",
        library_group=config["input"]["library_groups"].keys(),
    )
    # library_group_scaling_pairs = expand(
    #     f"{stats_library_group_folder}/{{library_group}}.{assembly}.scaling.tsv",
    #     library_group=config["input"]["library_groups"].keys(),
    # )

assemble_mapstats = []
assemble_pairstats = []
for library in LIBRARY_RUN_FASTQS:
    assemble_mapstats.append(
        f"{mapped_parsed_sorted_chunks_folder}/{library}/assemble.mapstat"
    )
    assemble_pairstats.append(
        f"{mapped_parsed_sorted_chunks_folder}/{library}/assemble.pairstat"
    )
multiqc = [f"{multiqc_folder}/multiqc_report.html"]
combined_dedup_stats = f"{pairs_library_folder}/combined_dedup_stats_summary.tsv"
combined_mapstats = f"{pairs_library_folder}/combined_mapstats_summary.tsv"
combined_pairstats = f"{pairs_library_folder}/combined_pairstats_summary.tsv"
combined_RSstats = f"{pairs_library_folder}/combined_RSstats_summary.tsv"
combined_hicproPairstats = f"{pairs_library_folder}/combined_hicpro_pairstats_summary.tsv"
combined_stats = [combined_dedup_stats, combined_mapstats, combined_pairstats,combined_RSstats,combined_hicproPairstats]

# Setting up resgen upload outputs (empty files indicating that the upload has been done)
resgen_uploads = (
    [f"{clr}_uploaded" for clr in library_coolers + library_group_coolers]
    if config["resgen"]["upload"]
    else []
)

allvalidPair = []
# allvalidpair
for library in LIBRARY_RUN_FASTQS:
    fastqc += expand(
        f"{pairs_library_folder}/{library}.allValidPairs"
    )

# Some sanity checks
# Phasing
if config.get('phase', {}).get('do_phase', False):
    if config['map']['mapper']=='chromap':
        raise ValueError(
            "Phasing is not possible with chromap, please use bwa-mem, bwa-mem2 or bwa-meme."
        )
    parse_options = argstring_to_dict(config['parse'].get('parsing_options', ''))
    if '--min-mapq' not in parse_options or parse_options['--min-mapq'] != '0':
        raise ValueError(
            "Plase set '--min-mapq to 0' in the parsing options to use phasing."
        )
    if '--add-columns' not in parse_options:
        raise ValueError(
            "Please set the appropriate --add-columns argument in the parsing options to use phasing."
        )
    elif config['phase']['tag_mode']=='XA' and not set(['XA', 'NM', 'AS', 'XS', 'mapq']).issubset(set(parse_options['--add-columns'].split(','))):
        raise ValueError(
            "Please set '--add-columns XA,NM,AS,XS,mapq' in the parsing options to use phasing with XA tag mode."
        )
    elif config['phase']['tag_mode']=='XB' and not set(['XB', 'NM', 'AS', 'XS', 'mapq']).issubset(set(parse_options['--add-columns'].split(','))):
        raise ValueError(
            "Please set '--add-columns XB,NM,AS,XS,mapq' in the parsing options to use phasing with XB tag mode."
        )
    # simpler check, should be enough and in case of multiple extra col pairs, where the dict would only record the last one...
    dedup_options = config["dedup"].get("dedup_options", "") 
    if '--extra-col-pair phase1 phase1' not in dedup_options or '--extra-col-pair phase2 phase2' not in dedup_options:
        raise ValueError(
            "Please add '--extra-col-pair phase1 phase2' in the dedup options to use phasing."
        )


# List of all required outputs of the workflow
# Note that pairs are not listed, since they are needed to produce coolers, so they are
# implicitly required
rule default:
    input:
        library_group_coolers,
        library_group_hic,
        library_group_stats,
        library_coolers,
        library_hic,
        library_scaling_pairs,
        # library_group_scaling_pairs,
        fastqc,
        multiqc,
        resgen_uploads,
        combined_stats,
        assemble_mapstats,
        allvalidPair,
        library_downstream_stat


# Global constraints on the wildcards to be used in the workflow - ensures correct
# parsing of all file names into wildcards
wildcard_constraints:
    library=f"({'|'.join([re.escape(lib) for lib in LIBRARY_RUN_FASTQS.keys()])})",
    library_group=(
        f"({'|'.join([re.escape(lib) for lib in config['input']['library_groups'].keys()])})"
        if "library_groups" in config["input"]
        else ""
    ),
    run=f"({'|'.join([re.escape(run) for run in runs])})",
    chunk_id="[0-9]+",


# Depending on the mapper, the index files will be different
if config["map"]["mapper"] == "bwa-mem":
    idx = multiext(
        genome_path,
        ".amb",
        ".ann",
        ".bwt",
        ".pac",
        ".sa",
    )

elif config["map"]["mapper"] == "bwa-mem2":
    idx = multiext(
        genome_path,
        ".0123",
        ".amb",
        ".ann",
        ".bwt.2bit.64",
        ".pac",
    )
elif config["map"]["mapper"] == "bowtie2":
    idx = multiext(
        bowtie_index_path,
        ".1.bt2",
        ".2.bt2",
        ".3.bt2",
        ".4.bt2",
        ".rev.1.bt2",
        ".rev.2.bt2",
    )

else:  # bwa-meme
    idx = multiext(
        genome_path,
        ".0123",
        ".amb",
        ".ann",
        ".pac",
        ".pos_packed",
        ".suffixarray_uint64",
        ".suffixarray_uint64_L0_PARAMETERS",
        ".suffixarray_uint64_L1_PARAMETERS",
        ".suffixarray_uint64_L2_PARAMETERS",
    )

if config["map"]["mapper"] == "chromap":

    ruleorder: map_chunks_chromap > parse_sort_chunks

elif config["map"]["mapper"] == "bowtie2":
    
#    ruleorder: pair_rescue_reads > map_chunks_bwa_pipe > map_chunks_bwa
    pass
else:

    ruleorder: parse_sort_chunks > map_chunks_chromap


include: "rules/resgen.smk"
include: "rules/pairtools.smk"
include: "rules/report.smk"
include: "rules/preprocess.smk"
include: "rules/align.smk"
include: "rules/hicpro_style.smk"
include: "rules/cooltools.smk"
include: "rules/downstream.smk"

checkpoint chunk_fastq:
    input:
        fastq1=lambda wildcards: (
            f"{downloaded_fastqs_folder}/{wildcards.library}.{wildcards.run}.1.fastq.gz"
            if needs_downloading(
                LIBRARY_RUN_FASTQS[wildcards.library][wildcards.run], 0
            )
            else LIBRARY_RUN_FASTQS[wildcards.library][wildcards.run][0]
        ),
        fastq2=lambda wildcards: (
            f"{downloaded_fastqs_folder}/{wildcards.library}.{wildcards.run}.2.fastq.gz"
            if needs_downloading(
                LIBRARY_RUN_FASTQS[wildcards.library][wildcards.run], 1
            )
            else LIBRARY_RUN_FASTQS[wildcards.library][wildcards.run][1]
        ),
    params:
        chunksize=lambda wildcards: config["map"]["chunksize"] * 4,  #*4 because we split by lines, not by reads
    threads: lambda wildcards: 4 if config["map"]["chunksize"] > 0 else 1
    conda:
        lambda wildcards: (
            "envs/download_fastqs.yml" if config["map"]["chunksize"] > 0 else None
        )
    output:
        #touch(f"{processed_fastqs_folder}/{{library}}/{{run}}/.chunks_ready")
        directory(f"{processed_fastqs_folder}/{{library}}/{{run}}"),
    log:
        "logs/chunk_runs/{library}.{run}.tsv",
    benchmark:
        "benchmarks/chunk_runs/{library}.{run}.tsv"
    shell:
        chunk_command

# rule make_chromsizes:
#     input:
#         genome_faidx=f"{config['input']['genome']['genome_fasta_path']}.fai",
#     output:
#         chromsizes=chromsizes_path,
#     shell:
#         "cut -f1,2 {input.genome_faidx} > {output.chromsizes}"

# If we keep bams, simply store the output in the folder, otherwise pipe it to pairtools parse
# Perhaps something with tee might be faster, had trouble getting it to work
if config["map"]["mapper"] in ["bwa-mem", "bwa-mem2", "bwa-meme"]:
    if config["parse"]["keep_unparsed_bams"]:

        ruleorder: map_chunks_bwa > map_chunks_bwa_pipe

    else:

        ruleorder: map_chunks_bwa_pipe > map_chunks_bwa

    use rule map_chunks_bwa as map_chunks_bwa_pipe with:
        output:
            pipe(
                f"{mapped_parsed_sorted_chunks_folder}/{{library}}/{{run}}/{{chunk_id}}.bam"
            ),



# use rule scaling_pairs_library as scaling_pairs_library_group with:
#     input:
#         pairs=f"{stats_library_group_folder}/{{library_group}}.{assembly}.pairs.gz",
#         chromsizes=chromsizes_path,
#     output:
#         scaling=f"{stats_library_group_folder}/{{library_group}}.{assembly}.scaling.tsv",
#     log:
#         "logs/scaling_pairs_library_group/{library_group}.log",
#     benchmark:
#         "benchmarks/scaling_pairs_library_group/{library_group}.tsv"


rule merge_stats_libraries_into_groups:
    input:
        lambda wildcards: expand(
            f"{pairs_library_folder}/{{library}}.{assembly}.dedup.stats",
            library=config["input"]["library_groups"][wildcards.library_group],
        ),
    conda:
        "envs/pairtools_cooler.yml"
    log:
        "logs/merge_stats_libraries_into_groups/{library_group}.log",
    output:
        f"{stats_library_group_folder}/{{library_group}}.{assembly}.stats",
    shell:
        r"pairtools stats --merge {input} -o {output} >{log[0]} 2>&1"
